# Develop an AI app with the Azure AI Foundry SDK

Use the Azure AI Foundry SDK to develop AI applications with Azure AI Foundry projects.

## Learning objectives
After completing this module, you'll be able to:

- Describe capabilities of the Azure AI Foundry SDK.
- Use the Azure AI Foundry SDK to work with connections in projects.
- Use the Azure AI Foundry SDK to develop an AI chat app.

## Introduction

Developers creating AI solutions with Azure AI Foundry need to work with a combination of services and software frameworks. The **Azure AI Foundry SDK** is designed to bring together common services and code libraries in an AI project through a central programmatic access point, making it easier for developers to write the code needed to build effective AI apps on Azure.

In this module, you'll learn how to use the Azure AI Foundry SDK to work with resources in an AI project.

## Summary

By using the **Azure AI Foundry SDK**, you can develop rich AI applications that use resources in your Azure AI Foundry projects. **The Azure AI Foundry SDK AIProjectClient class provides a programmatic proxy for a project**, enabling you to access connected resources and to use service-specific libraries to consume them.

---

## What is the Azure AI Foundry SDK?

**The Azure AI Foundry SDK is a set of packages and services designed to work together to enable developers to write code that uses resources in an Azure AI Foundry project**. With the Azure AI Foundry SDK, developers can create applications that connect to a project, access the resource connections and models in that project, and use them to perform AI operations, such as sending prompts to a generative AI model and processing the responses

The SDK provides Python and Microsoft C# .NET libraries that you can use to build AI applications based on Azure AI Foundry projects.

### Installing SDK packages

The core package for working with projects in the Azure AI Foundry SDK is the **Azure AI Projects library**, which enables you to connect to an Azure AI Foundry project and access the resources defined within it.

To use the Azure AI Projects library in C#, add the Azure.AI.Projects package to your C# project:

`dotnet add package Azure.AI.Projects --prerelease`

### Using the SDK to connect to a project

**The first task in most Azure AI Foundry SDK code is to connect to an Azure AI Foundry project**. Each project has a unique connection string, which you can find on the project's Overview page in the Azure AI Foundry portal.

You can use that connection string in your code to create an **AIProjectClient** object, which provides a programmatic proxy for the project.

The following code snippet shows how to create am AIProjectClient object in C#.

`using Azure.Identity;`
`using Azure.AI.Projects;`
`...`
`var connectionString = "<region>.api.azureml.ms;<project_id>;<hub_name>;<project_name>";`
`var projectClient = new AIProjectClient(connectionString, new DefaultAzureCredential());`

Note: The code uses the default Azure credentials to authenticate when accessing the project. To enable this authentication, in addition to the Azure.AI.Projects package, you need to install the Azure.Identity package:

`dotnet add package Azure.Identity`

Tip: To access the project successfully, the code must be run in the context of an authenticated Azure session. For example, you could use the Azure command-line interface (CLI) `az-login` command to sign in before running the code.

---

## Work with project connections

Each Azure AI Foundry project includes connected resources, which are defined both at the hub and project level. Each resource is a connection to an external service, such as Azure AI services, Azure storage, Azure AI search, and others.

With the Azure AI Foundry SDK, you can connect to a project and retrieve connections; which you can then use to consume the connected services.

The `AIProjectClient` object in C# has a `GetConnectionsClient()` method, which returns a `ConnectionsClient` object that you can use to access the resource connections in the project. Methods of the `ConnectionsClient` object include:

- `GetConnections()`: Returns a collection of `ConnectionResponse` objects, each representing a connection in the project. You can filter the results by specifying an optional `ConnectionType` parameter with a valid enumeration, such as `ConnectionType.AzureAIServices`.
- `GetConnection(connectionName)`: Returns a connection object for the connection with the name specified.
- `GetDefaultConnection(connectionType)`: Returns the default connection of the specified type - for example, the default Azure AI services connection defined in the project.

The connection objects returned by these methods include connection-specific properties, which you can use to connect to the associated resource. For example, the following code snippet retrieves the default Azure AI services connection and uses it to instantiate a `TextAnalyticsClient` object in order to use the sentiment analysis functionality of the Azure AI Language service (which is included in an Azure AI services resource).

Note: In addition to the Azure.AI.Projects and Azure.Identity packages discussed previously, the sample code shown here assumes that the Azure.AI.TextAnalytics package has been installed:

`dotnet add package Azure.AI.TextAnalytics`

---

## Create a chat client

A common scenario in an AI application is to **connect to a generative AI model and use prompts to engage in a chat-based dialog with it**. You can use the Azure AI Foundry SDK to chat with models that you have deployed in your Azure AI Foundry project.

The specific libraries and code used to build a chat client depends on how the target model has been deployed in the Azure AI Foundry project. You can deploy models to the following model hosting solutions:

- **Azure AI model inference**: _A single endpoint for multiple models of different types, including OpenAI models and others from the Azure AI Foundry model catalog_. Models are consumed through an Azure AI services resource connection in the project.
- **Azure OpenAI service**: _A single endpoint for OpenAI models hosted in Azure_. Models are consumed through an Azure OpenAI service resource connection in the project.
- **Serverless API**: A model-as-a-service solution in which each deployed model is **accessed through a unique endpoint and hosted in the Azure AI Foundry project**.
- **Managed compute**: A model-as-a-service solution in which each deployed model is **accessed through a unique endpoint hosted in custom compute**.

Note: To deploy models to an Azure AI model inference endpoint, you must enable the Deploy models to Azure AI model inference service option in Azure AI Foundry.

In this module, we'll focus on models deployed to the **Azure AI model inference service** and models deployed to the **Azure OpenAI service**.

### Building a client app for Azure AI model inference deployments

When you have deployed models to the Azure AI model inference service, you can use the Azure AI Foundry SDK to write code that creates a `ChatCompletionsClient` object, which you can then use to chat with a deployed model. One of the benefits of using this model deployment type is that you can easily **switch between deployed models by changing one parameter in your code (the model deployment name)**, making it a great way to test against multiple models while developing an app.

The following C# code sample uses a ChatCompletionsClient object to chat with a model deployment named phi-4-model.

using System;
using Azure;
using Azure.AI.Projects;
using Azure.Identity;
using Azure.AI.Inference;

namespace my_foundry_client
{
    class Program
    {
        static void Main(string[] args)
        {
            try
            {
                // Initialize the project client
                var connectionString = "<region>.api.azureml.ms;<project_id>;<hub_name>;<project_name>";
                var projectClient = new AIProjectClient(connectionString, new DefaultAzureCredential());

                // Get a chat client
                ChatCompletionsClient chatClient = projectClient.GetChatCompletionsClient();

                // Get a chat completion based on a user-provided prompt
                Console.WriteLine("Enter a question:");
                var user_prompt = Console.ReadLine();

                var requestOptions = new ChatCompletionsOptions()
                {
                    Model = "phi-4-model",
                    Messages =
                        {
                            new ChatRequestSystemMessage("You are a helpful AI assistant that answers questions."),
                            new ChatRequestUserMessage(user_prompt),
                        }
                };

                Response<ChatCompletions> response = chatClient.Complete(requestOptions);
                Console.WriteLine(response.Value.Content);
            }
            catch (Exception ex)
            {
                Console.WriteLine(ex.Message);
            }
        }
    }
}

Note: The `ChatCompletionsClient` class uses `Azure AI Inference` library. In addition to the `Azure.AI.Projects` and `Azure.Identity` packages discussed previously, the sample code shown here assumes that the Azure.AI.Inference package has been installed:

`dotnet add package Azure.AI.Inference`

### Building a client app for Azure OpenAI service deployments

When you have deployed models to the Azure OpenAI service, you can use the `AIProjectConnection` to connect to the Azure OpenAI service resource in your project and then use the Azure OpenAI SDK to chat with your models.

To chat with a model that is deployed to the Azure OpenAI service in an Azure AI Foundry project from C#, use the `AIProjectClient` object to retrieve the default Azure OpenAI resource connection from the project, and then use the Azure OpenAI SDK to connect to the service and send prompts to the model.

The following C# code sample uses the Azure AI Foundry and Azure OpenAI SDKs to chat with a model deployment named gpt-4-model.

Note: In addition to the Azure.AI.Projects and Azure.Identity packages discussed previously, the sample code shown here assumes that the Azure.AI.OpenAI package has been installed:

`dotnet add package Azure.AI.OpenAI`

---

## Exercise

https://microsoftlearning.github.io/mslearn-ai-studio/Instructions/02a-AI-foundry-sdk.html

---

## Module assessment

1. What class in the Azure AI Foundry SDK provides a proxy object for a project? AIProjectClient
2. What value is needed to instantiate a AIProjectClient object? The project connection string.
3. Which library should you use to chat with a model that is deployed to the Azure AI model inference service? Azure AI Inference