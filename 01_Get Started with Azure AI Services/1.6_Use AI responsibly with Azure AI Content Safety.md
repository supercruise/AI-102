# Use AI responsibly with Azure AI Content Safety

https://learn.microsoft.com/en-us/training/modules/responsible-content-safety/

Azure AI Content Safety is a comprehensive tool designed to **detect and manage harmful content** in both user-generated and AI-generated materials. Learn how Azure AI Content Safety uses text and image APIs to help **identify and filter out content related to violence, hate, sexual content, and self-harm**.

## Learning objectives
By the end of this module, you'll be able to:

- Describe Azure AI Content Safety.
- Describe how Azure AI Content Safety operates.
- Describe when to use Azure AI Content Safety.

---

## Introduction

The amount of user-generated content being posted online is growing rapidly. We are also increasingly aware of the need to protect everyone from inappropriate or harmful content.

**Azure AI Content Safety is an AI service designed to help developers include advanced content safety into their applications and services.**

The challenges in maintaining safe and respectful online spaces are growing for developer teams responsible for hosting online discussions. Azure AI Content Safety identifies potentially unsafe content and helps organizations to comply with regulations and meet their own quality standards.

The need for improving online content safety has four main drivers:

- **Increase in harmful content**: There's been a huge growth in user-generated online content, including harmful and inappropriate content.
- **Regulatory pressures**: Government pressure to regulate online content.
- **Transparency**: Users need transparency in content moderation standards and enforcement.
- **Complex content**: Advances in technology are making it easier for users to post multimodal content and videos.

Note: Azure AI Content Safety replaces Azure Content Moderator, which was deprecated in February 2024 and will be retired by February 2027.

In this module, you'll learn about the key features of Azure AI Content Safety, and when each might be used.

---

## What is Content Safety

Azure AI Content Safety is a set of advanced content moderating features that can be incorporated into your applications and services. Azure AI Content Safety is available as a resource in the Azure portal.

Online content safeguarding is needed in a growing number of situations. Not only are we concerned with moderating content generated by people, but must also guard against the malicious use of AI.

### Trusting user-generated content

Social interaction is increasingly a part of many digital spaces. Genuine user-generated content is seen as independent and trustworthy, and used alongside advertising and marketing. Different industries are encouraging their customers to connect with each other and their brand.

Harmful content has many negative effects. It damages trusted brands, discourages users from participating in online forums, and can have a devastating impact on individuals.

**Azure AI Content Safety is designed to be used in applications and services to protect against harmful user-generated and AI-generated content.**

### Content Safety in Azure AI Foundry

Azure [AI Content Safety](https://ai.azure.com/explore/contentsafety) is available as part of Azure AI Foundry, a unified platform that enables you to explore many different Azure AI services, including Content Safety.

From the Azure AI Foundry home page, scroll down and select Explore Azure AI Services. From here, you can explore Content Safety by selecting View all Content Safety capabilities.

**Azure AI Content Safety Studio** enables you to explore and test Content Safety features for yourself. Select the feature you want to try, and then select Try it out. You can then use the user interface to test samples or your own material. Select View code to generate sample code in C#, Java, or Python. You can then copy and paste the sample code and amend the variables to use your own data.

---

## How does Azure AI Content Safety work?

**Azure AI Content Safety works with text and images, and AI-generated content.**

Content Safety vision capabilities are powered by Microsoft's Florence foundation model, which has been trained with billions of text-image pairs. Text analysis uses natural language processing techniques, giving a better understanding of nuance and context. Azure AI Content Safety is multilingual and can detect harmful content in both short form and long form. It's currently available in English, German, Spanish, French, Portuguese, Italian, and Chinese.

Azure AI Content Safety classifies content into **four categories**:

- **Hate**
- **Sexual**
- **Self-harm**
- **Violence**

A severity level for each category is used to determine whether content should be blocked, sent to a moderator, or auto approved.

Azure AI Content Safety features include:

### Safeguarding text content

- **Moderate text** scans text across **four categories**: *violence, hate speech, sexual content, and self-harm*. **A severity level from 0 to 6** is returned for each category. This level helps to prioritize what needs immediate attention by people, and how urgently. You can also create a blocklist to scan for terms specific to your situation.

- **Prompt shields** is a unified API to identify and block jailbreak attacks from inputs to LLMs. It includes both user input and documents. These attacks are prompts to LLMs that attempt to bypass the model's in-built safety features. User prompts are tested to ensure the input to the LLM is safe. Documents are tested to ensure they don't contain unsafe instructions embedded within the text.

- **Protected material detection** checks AI-generated text for protected text such as recipes, copyrighted song lyrics, or other original material.

- **Groundedness detection** protects against inaccurate responses in AI-generated text by LLMs. Public LLMs use data available at the time they were trained. However, data can be introduced after the original training of the model or be built on private data. A **grounded response** is one where *the modelâ€™s output is based on the source information*. An **ungrounded response** is one where *the model's output varies from the source information*. Groundedness detection includes a reasoning option in the API response. This adds a reasoning field that explains any ungroundedness detection. However, reasoning increases processing time and costs.

### Safeguarding image content

- **Moderate images** scans for inappropriate content across **four categories**: *violence, self-harm, sexual, and hate*. A **severity level** is returned: *safe, low, or high*. You then set a **threshold level** of *low, medium, or high*. The combination of the severity and threshold level determines whether the image is allowed or blocked for each category.

- **Moderate multimodal content** scans both images and text, including text extracted from an image using optical character recognition (OCR). Content is analyzed across **four categories**: *violence, hate speech, sexual content, and self-harm*.

### Custom safety solutions

- **Custom categories** enables you to create your own categories by *providing positive and negative examples*, and training the model. Content can then be scanned according to your own category definitions.

- **Safety system message** helps you to write effective prompts to guide an AI system's behavior.

### Limitations

Foundry Content Safety uses AI algorithms, and so may not always detect inappropriate language. And on occasions it might block acceptable language because it relies on algorithms and machine learning to detect problematic language.

Foundry Content Safety should be *tested and evaluated on real data before being deployed*. And once deployed, you should *continue to monitor the system* to see how accurately it's performing.

### Evaluating accuracy

When evaluating how accurately Foundry Content Safety is for your situation, compare its performance against **four criteria**:

- **True positive** - correct identification of harmful content.
- **False positive** - incorrect identification of harmful content.
- **True negative** - correct identification of harmless content.
- **False negative** - harmful content isn't identified.

Foundry Content Safety works best to support human moderators who can resolve cases of incorrect identification. When people add content to a site, they don't expect posts to be removed without reason. Communicating with users about why content is removed or flagged as inappropriate helps everyone to understand what is permissible and what isn't.

---

## [When to use Azure AI Foundry Content Safety](https://learn.microsoft.com/en-us/training/modules/responsible-content-safety/4-when-to-use-content-safety)

Many online sites encourage users to share their views. People trust other people's feedback about products, services, brands, and more. These comments are often frank, insightful, and seen to be free of marketing bias. But not all content is well intended.

**Azure AI Foundry Content Safety** is an AI service designed to provide a more comprehensive approach to content moderation. Foundry Content Safety helps organizations to prioritize work for human moderators in a growing number of situations:

### Education
The number of learning platforms and online educational sites is growing rapidly, with more and more information being added all the time. Educators need to be sure that students aren't being exposed to inappropriate content, or inputting harmful requests to LLMs. In addition, both educators and students want to know that the content they're consuming is correct and close to the source material.

### Social
Social media platforms are dynamic and fast moving, requiring real-time moderation. Moderation of user-generated content includes posts, comments, and images. Foundry Content Safety helps moderate content that is nuanced and multi-lingual to identify harmful material.

### Brands
Brands are making more use of chat rooms and message forums to encourage loyal customers to share their views. However offensive material can damage a brand, and discourage customers from contributing. They want to be assured that inappropriate material can be quickly identified and removed. Brands are also adding generative AI services to help people to communicate with them, and therefore need to guard against bad actors attempting to exploit large language models (LLMs).

### E-Commerce
User content is generated by reviewing products and discussing products with other people. This material is powerful marketing, but when inappropriate content is posted it damages consumer confidence. In addition, regulatory and compliance issues are increasingly important. Foundry Content Safety helps screen product listings for fake reviews and other unwanted content.

### Gaming
Gaming is a challenging area to moderate due to its highly visual and often violent graphics. Gaming has strong communities where people are enthusiastic about sharing progress and their experiences. Supporting human moderators to keep gaming safe includes monitoring avatars, usernames, images, and text-based materials. Foundry Content Safety has advanced AI vision tools to help moderate gaming platforms to detect misconduct.

### Generative AI services
Organizations are increasingly using generative AI services to enable internal data to be accessed more easily. To maintain the integrity and safety of internal data, both user prompts and AI-generated outputs need to be checked to prevent malicious use of these systems.

### News
News websites need to moderate user comments to prevent the spread of misinformation. Foundry Content Safety can identify language that includes hate speech and other harmful content.

### Other situations
There are many other situations where content needs to be moderated. Foundry Content Safety can be customized to identify problematic language for specific cases.

---

## [Exercise - Implementing Azure AI Foundry Content Safety](https://learn.microsoft.com/en-us/training/modules/responsible-content-safety/5-exercise-content-safety)

### [Implement Azure AI Content Safety](https://microsoftlearning.github.io/mslearn-ai-services/Instructions/Exercises/05-implement-content-safety.html)

In this exercise, you will provision a Content Safety resource, test the resource in Azure AI Studio, and test the resource in code.

---

## Module assessment

1. Which feature of Azure AI Foundry Content Safety helps protect large language models from document injection attacks? Prompt Shields
2. What is the purpose of the Groundedness detection feature in Foundry Content Safety? To verify AI-generated text is based on provided source materials.
3. Which social media issues does Foundry Content Safety address? The growth of inappropriate online content including bullying and hate speech.
4. How does Foundry Content Safety help businesses to protect their brand image? By moderating comments and messages from customers.
5. What is a benefit of Foundry Content Safety? Reducing the amount of psychologically damaging material that human moderators are exposed to.

---

## Summary

The proliferation of user-generated content makes it near-impossible for human moderators to effectively manage online platforms. Yet as the amount of user-generated content grows, so does the importance of online safety.

**Azure AI Foundry Content Safety uses AI models to automatically detect violent, sexual, self-harm, or hateful language in real time**. It allocates a severity level, so that human moderators can focus on high-priority cases and be exposed to a smaller amount of disturbing content. Foundry Content Safety includes features to moderate both people-generated and AI-generated material.

In this module, you've seen how the features of Foundry Content Safety can help e-commerce brands, gaming companies, and educators to provide safer spaces for users.