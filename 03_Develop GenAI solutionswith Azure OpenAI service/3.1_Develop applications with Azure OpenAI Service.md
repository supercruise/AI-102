# Develop applications with Azure OpenAI Service

This module provides engineers with the skills to begin building an Azure OpenAI Service solution.

## Learning objectives
By the end of this module, you'll be able to:

- Create an Azure OpenAI Service resource and understand types of Azure OpenAI base models.
- Build natural language solutions with Azure OpenAI Service
- Apply **prompt engineering** with Azure OpenAI Service
- Build **code-generation** solutions with Azure OpenAI Service

## Introduction

Suppose you want to build a support application that summarizes text and suggest code. To build this app, you want to utilize the capabilities you see in ChatGPT, a chatbot built by the OpenAI research company that takes in natural language input from a user and returns a machine-created, human-like response.

Generative AI models power ChatGPT's ability to produce new content, such as text, code, and images, based on natural language prompts. Many generative AI models are a subset of [deep learning algorithms](https://learn.microsoft.com/en-us/dotnet/machine-learning/deep-learning-overview). These algorithms support various workloads across **vision, speech, language, decision, search**, and more.

Azure OpenAI Service brings these generative AI models to the Azure platform, enabling you to develop powerful AI solutions that benefit from the security, scalability, and integration of other services provided by the Azure cloud platform. These models are available for **building applications through a REST API, various SDKs, and a Studio interface**. This module guides you through the Azure AI Foundry experience, giving you the foundation to further develop solutions with generative AI.

---

## Access Azure OpenAI Service

The first step in building a generative AI solution with Azure OpenAI is to **provision an Azure OpenAI resource in your Azure subscription**. You can get started by creating a resource in the [Azure portal](https://portal.azure.com/) or with the Azure command line interface (CLI).

### Create an Azure OpenAI Service resource in the Azure portal

When you create an Azure OpenAI Service resource, you need to provide **a subscription name, resource group name, region, unique instance name, and select a pricing tier**.

### Create an Azure OpenAI Service resource in Azure CLI

To create an Azure OpenAI Service resource from the CLI, refer to this example and replace the following variables with your own:

- MyOpenAIResource: replace with a unique name for your resource
- OAIResourceGroup: replace with your resource group name
- eastus: replace with the region to deploy your resource
- subscriptionID: replace with your subscription ID

`az cognitiveservices account create \
-n MyOpenAIResource \
-g OAIResourceGroup \
-l eastus \
--kind OpenAI \
--sku s0 \
--subscription subscriptionID`

### Use Azure OpenAI Foundry

Azure AI Foundry provides access to **model management, deployment, experimentation, customization, and learning resources**.

You can access the Azure AI Foundry through the Azure portal after creating a resource, or at https://ai.azure.com/ by signing in to your Azure account. During the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.

When you first open Azure AI Foundry, you'll want to navigate to the Azure OpenAI page (where you focus on only Azure OpenAI Service models), select your resource if you haven't already, and deploy your first model. To do so, select the Deployments page, from where you can deploy a base model and start experimenting with it.

> Note: If you are not the resource owner, you will need the following role-based access controls: 1. Cognitive Services OpenAI User: This role allows viewing resources and using the chat playground. 2. Cognitive Services OpenAI Contributor: This role allows the user to create new deployments.

### Types of OpenAI models

To begin building with Azure OpenAI, you need to **choose a base model and deploy it**. Microsoft provides base models and the option to create customized base models. This unit covers the currently available base models.

Azure OpenAI includes several types of model:

- **GPT-4 models** are the latest generation of *generative pretrained (GPT)* models that can generate natural language and code completions based on natural language prompts.
- **GPT 3.5 models** can generate natural language and code completions based on natural language prompts. In particular, GPT-35-turbo models are optimized for chat-based interactions and work well in most generative AI scenarios.
- **Embeddings models** convert text into numeric vectors, and are useful in language analytics scenarios such as comparing text sources for similarities.
- **DALL-E models** are used to generate images based on natural language prompts. Currently, DALL-E models are in preview.
- **Whisper models** are used to convert speech to text.
- **Text to speech models** are used to convert text to speech.

> Note: Pricing is determined by tokens and by model type. Learn more about the latest [pricing here](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/).

In the Azure AI Foundry portal, the **Model Catalog** page lists the available base models and provides an option to create additional customized models by fine-tuning the base models. The models that have a *Succeeded* status mean they're successfully trained and can be selected for deployment.

You'll notice that there are various models beyond OpenAI available in the Model Catalog, including models from Microsoft, Meta, Mistral, and more. Azure AI Foundry enables you to deploy any of these models for your use case. This module will focus on Azure OpenAI models.

### Deploying OpenAI models

You first need to deploy a model to chat with or make API calls to receive responses to prompts. When you create a new deployment, you need to indicate which base model to deploy. You can deploy any number of deployments as long as their **Tokens Per Minute (TPM)** stays within the deployment quota.

#### Deploy using Azure AI Foundry

In Azure AI Foundry portal's **Model catalog** page, you can create a new deployment by selecting a model name from the list.

#### Deploy using Azure CLI

You can also deploy a model using the console. Using this example, replace the following variables with your own resource values:

- OAIResourceGroup: replace with your resource group name
- MyOpenAIResource: replace with your resource name
- MyModel: replace with a unique name for your model
- gpt-35-turbo: replace with the base model you wish to deploy

`az cognitiveservices account deployment create \
   -g OAIResourceGroup \
   -n MyOpenAIResource \
   --deployment-name MyModel \
   --model-name gpt-35-turbo \
   --model-version "0125"  \
   --model-format OpenAI \
   --sku-name "Standard" \
   --sku-capacity 1`

#### Deploy using the REST API

You can deploy a model using the **REST API**. In the request body, you specify the base model you wish to deploy. See an example in the [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/).

### Explore OpenAI prompts

Once the model is deployed, you can test how it completes prompts. A prompt is the text portion of a request that is sent to the deployed model's completions endpoint. Responses are referred to as *completions*, which can come in form of text, code, or other formats.

#### Prompt types
Prompts can be grouped into types of requests based on task.

| Task type | Prompt example | Completion example |
| -- | -- | -- |
|Classifying content|Tweet: I enjoyed the trip. **Sentiment**:|Positive|
|Generating new content|List ways of traveling|1. Bike; 2. Car ...|
|Holding a conversation	| A friendly AI assistant | See examples https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/completions#conversation?portal=true |
|Transformation (translation and symbol conversion)|English: Hello; French:|bonjour|
|Summarizing content|Provide a summary of the content {text}|The content shares methods of machine learning.|
|Picking up where you left off|One way to grow tomatoes|is to plant seeds.|
|Giving factual responses|How many moons does Earth have?|One|

#### Completion quality

Several factors affect the quality of completions you'll get from a generative AI solution.

- The way a prompt is engineered. Learn more about prompt engineering in the *Prompt engineering* unit, later in this module.
- The model parameters (covered below).
- The data the model is trained on, which can be adapted through [model fine-tuning with customization](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/fine-tuning?pivots=programming-language-studio?portal=true).

You have more control over the completions returned by training a custom model than through prompt engineering and parameter adjustment.

#### Making API calls

You can start making calls to your deployed model via the REST API, Python, or C#. If your deployed model has a GPT-3.5 or GPT-4 model base, use the [Chat completions documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt), which has endpoints and variables required for these base models.

### Use the Azure Studio playgrounds

Playgrounds are useful interfaces in Azure AI Foundry that you can use to experiment with your deployed models without needing to develop your own client application. Azure AI Foundry portal offers multiple playgrounds with different parameter tuning options.

#### Chat playground

The Chat playground is based on a **conversation-in, message-out** interface. You can initialize the session with a system message to set up the chat context.

In the Chat playground, you're able to use prompt samples, adjust parameters, and add *few-shot examples*. The term few-shot refers to providing a few of examples to help the model learn what it needs to do. You can think of it in contrast to *zero-shot*, which refers to providing no examples.

![alt text](https://learn.microsoft.com/en-us/training/wwl-data-ai/develop-applications-openai/media/studio-chat-playground.png)

---

## Integrate OpenAI into an app

Azure OpenAI offers both **language specific SDKs** and a **REST API** that developers can use to add AI functionality to their applications. Generative AI capabilities in Azure OpenAI are provided through models. The models available in the Azure OpenAI service belong to different families, each with their own focus. To use one of these models, you need to deploy through the Azure OpenAI Service.

Once you have created an Azure OpenAI resource and deployed a model, you can configure your app.

### Available endpoints

Azure OpenAI can be accessed via a REST API or an SDK available for Python, C#, JavaScript, and more. The endpoints available for interacting with a deployed model are used differently, and certain endpoints can only use certain models. The available endpoints are:

- **Completion** - **model takes an input prompt**, and generates one or more predicted completions. You'll see this playground in the studio, but won't be covered in depth in this module.
- **ChatCompletion** - **model takes input in the form of a chat conversation** (where roles are specified with the message they send), and the next chat completion is generated.
- **Embeddings** - model takes input and **returns a vector representation** of that input.

For example, the input for `ChatCompletion` is a conversation with clearly defined roles for each message:

`{"role": "system", "content": "You are a helpful assistant, teaching people about AI."},`
`{"role": "user", "content": "Does Azure OpenAI support multiple languages?"},`
`{"role": "assistant", "content": "Yes, Azure OpenAI supports several languages, and can translate between them."},`
`{"role": "user", "content": "Do other Azure AI Services support translation too?"}`

When you give the AI model a real conversation, it can generate a better response with more accurate tone, phrasing, and context. The `ChatCompletion` endpoint enables the model to have a more realistic conversation by sending the history of the chat with the next user message.

`ChatCompletion` also allows for non-chat scenarios, such as summarization or entity extraction. This can be accomplished by providing a short conversation, specifying the system information and what you want, along with the user input. For example, if you want to generate a job description, provide ChatCompletion with something like the following conversation input.

`{"role": "system", "content": "You are an assistant designed to write intriguing job descriptions."},`
`{"role": "user", "content": "Write a job description for the following job title: 'Business Intelligence Analyst'. It should include responsibilities, required qualifications, and highlight benefits like time off and flexible hours."}`

>  Note: `Completion` is available for earlier `gpt-3` generation models, while `ChatCompletion` is the only supported option for `gpt-4` models and is the preferred endpoint when using the `gpt-35-turbo` model.

### Use the Azure OpenAI REST API

Azure OpenAI offers a REST API for interacting and generating responses that developers can use to add AI functionality to their applications. This unit covers example usage, input and output from the API.

For each call to the REST API, you need the **endpoint** and a **key** from your Azure OpenAI resource, and the name you gave for your deployed model. In the following examples, the following placeholders are used:

| Placeholder name | Value |
| -- | -- |
| YOUR_ENDPOINT_NAME| This base endpoint is found in the Keys & Endpoint section in the Azure portal. It's the base endpoint of your resource, such as https://sample.openai.azure.com/. |
| YOUR_API_KEY | Keys are found in the Keys & Endpoint section in the Azure portal. You can use either key for your resource. |
| YOUR_DEPLOYMENT_NAME | This deployment name is the name provided when you deployed your model in the Azure AI Foundry. |

#### Chat completions

Once you've deployed a model in your Azure OpenAI resource, you can send a prompt to the service using a POST request.

`curl https://YOUR_ENDPOINT_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-03-15-preview \
  -H "Content-Type: application/json" \
  -H "api-key: YOUR_API_KEY" \
  -d '{"messages":[{"role": "system", "content": "You are a helpful assistant, teaching people about AI."},
{"role": "user", "content": "Does Azure OpenAI support multiple languages?"},
{"role": "assistant", "content": "Yes, Azure OpenAI supports several languages, and can translate between them."},
{"role": "user", "content": "Do other Azure AI Services support translation too?"}]}'`

REST endpoints allow for specifying other optional input parameters, such as temperature, max_tokens and more. If you'd like to include any of those parameters in your request, add them to the input data with the request.

#### Embeddings

Embeddings are helpful for specific formats that are easily consumed by machine learning models. To generate embeddings from the input text, `POST` a request to the `embeddings` endpoint.

When generating embeddings, be sure to use a model in Azure OpenAI meant for embeddings. Those models start with `text-embedding` or `text-similarity`, depending on what functionality you're looking for.

The response from the API will be similar to the following JSON:

### Use Azure OpenAI with SDKs

In addition to REST APIs, users can also access Azure OpenAI models through C# and Python SDKs. The same functionality is available through both REST and these SDKs.

For both SDKs covered in this unit, you need the endpoint and a key from your Azure OpenAI resource, and the name you gave for your deployed model. In the following code snippets, the following placeholders are used:

| Placeholder name|Value|
| -- | -- |
| YOUR_ENDPOINT_NAME | This base endpoint is found in the Keys & Endpoint section in the Azure portal. It's the base endpoint of your resource, such as https://sample.openai.azure.com/. |
| YOUR_API_KEY | Keys are found in the Keys & Endpoint section in the Azure portal. You can use either key for your resource. |
| YOUR_DEPLOYMENT_NAME | This deployment name is the name provided when you deployed your model. |

#### Install libraries

First, install the client library for your preferred language. The C# SDK is a .NET adaptation of the REST APIs and built specifically for Azure OpenAI, however it can be used to connect to Azure OpenAI resources or non-Azure OpenAI endpoints. The Python SDK is built and maintained by OpenAI.

#### Configure app to access Azure OpenAI resource

Configuration for each language varies slightly, but both require the same parameters to be set. The necessary parameters are `endpoint`, `key`, and `deployment name`.

Add the library to your app, and set the required parameters for your client.

#### Call Azure OpenAI resource

Once you've configured your connection to Azure OpenAI, send your prompt to the model.

The response object contains several values, such as total_tokens and finish_reason. The completion from the response object will be similar to the following completion:

In both C# and Python, your call can include optional parameters including **temperature** and **max_tokens**.

---

## OpenAI prompt engineering

The quality of the input prompts we send to an AI model, like those available in Azure OpenAI, directly influences the quality of what we get back. By carefully constructing the prompts we send to the model, the model can provide better and more interesting responses.

### What is prompt engineering

Prompt engineering is **the process of designing and optimizing prompts to better utilize AI models**. Designing effective prompts is critical to the success of prompt engineering, and it can significantly improve the AI model's performance on specific tasks. Providing relevant, specific, unambiguous, and well structured prompts can help the model better understand the context and generate more accurate responses.

For example, if we want an OpenAI model to generate product descriptions, we can provide it with a detailed description that describes the features and benefits of the product. By providing this context, the model can generate more accurate and relevant product descriptions.

**Prompt engineering can also help mitigate bias and improve fairness in AI models**. By designing prompts that are diverse and inclusive, we can ensure that the model isn't biased towards a particular group or perspective.

> Important: No matter how good of a prompt you can design, responses from AI models should never be taken as fact or completely free from bias. Always use AI responsibly. For more information, see Microsoft's [transparency note](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note) on Azure OpenAI and [Microsoft's AI principles](https://www.microsoft.com/ai/responsible-ai).

In addition, prompt engineering can help us understand which references the model uses to generate its response. Generative AI models have a ton of parameters and the logic it follows is largely unknown to users, so it can be confusing how it arrives at the response it gives. By designing prompts that are easy to understand and interpret, we can help humans better understand how the model is generating its responses. This can be particularly important in domains such as healthcare, where it's critical to understand how the model is making decisions.

There are different methods to utilize when engineering your own prompts. These include **providing instructions, contextual content, cues or few-shot examples, and correctly ordering content in your prompt**. The methods covered here aren't exhaustive as this area is a nuanced and fluid topic.

#### Considerations for API endpoints

The examples in the rest of this unit will be focused on `ChatCompletion`. It's worth noting that `ChatCompletion` can also be used for non chat scenarios, where any instructions are included in the system message and user content is provided in the user role message. Most of these examples can be altered to use the `Completion` endpoint, if desired.

In terms of model availability, the `Completion` endpoint can be used with `gpt-3` and earlier, and `ChatCompletion` can be used with `gpt-35-turbo` and later models.

#### Adjusting model parameters

In addition to techniques discussed in this unit, adjusting parameters of the model can have a significant impact on the response. In particular, `temperature` and `top_p` (top_probability) are the most likely to impact a model's response as they both control randomness in the model, but in different ways.

**Higher values produce more creative and random responses**, but will likely be less consistent or focused. Responses expected to be fictional or unique benefit from higher values for these parameters, whereas **content desired to be more consistent and concrete should use lower values**.

Specifically, **high temperature allows for more variation in sentence structure** and **high top_p allows for more variation in words that are used** (using a variety of synonyms).

Try adjusting these parameters with the same prompt to see how they impact the response. **It's recommended to change either temperature or top_p at a time, but not both**.

### Write effective prompts

Azure OpenAI models are capable of generating responses to natural language queries with remarkable accuracy. However, the quality of the responses depends largely on how well the prompt is written. Developers can optimize the performance of Azure OpenAI models by using different techniques in their prompts, resulting in more accurate and relevant responses.

#### Provide clear instructions

Asking the Azure OpenAI model clearly for what you want is one way to get desired results. By being as descriptive as possible, the model can generate a response that most closely matches what you're looking for.

For example, say you want to create a product description for a new water bottle. Consider the answer associated with each prompt:

`Write a product description for a new water bottle`

Versus a similar prompt, with clear instructions.

`Write a product description for a new water bottle that is 100% recycled. Be sure to include that it comes in natural colors with no dyes, and each purchase removes 10 pounds of plastic from our oceans`

Including specifics that need to be included lets the model know what information to add to the description, making it more accurate to the new product.

This method can be extrapolated to include complex instructions, such as **a bulleted list of details to include, length of response, or desired formats to be included** in the output. Try asking for exactly what you want to see in the result, and you may be surprised at how well the model satisfies your request.

#### Format of instructions

How instructions are formatted can impact how the model interprets the prompt. Recency bias can affect models, where information located towards the end of the prompt can have more influence on the output than information at the beginning. You may get better responses by repeating the instructions at the end of the prompt and assessing how that affects the generated response.

This recency bias can also come into play when using `ChatCompletion` in a chat scenario, where **more recent messages in the conversation included in the prompt have a greater impact on the response**.

#### Primary, supporting, and grounding content

Including content for the model to use in its response allows the model to answer with greater accuracy. This content can be thought of in two ways: **primary and supporting content**.

**Primary content refers to content that is the subject of the query, such as a sentence to translate or an article to summarize**. This content is often included at the beginning or end of the prompt (as an instruction and differentiated by --- blocks), with instructions explaining what to do with it.

For example, say we have a long article that we want to summarize. We could put it in a --- block in the prompt, then end with the instruction.

`---`
`<insert full article here, as primary content>`
`---`
`Summarize this article and identify three takeaways in a bulleted list`

**Supporting content is content that may alter the response, but isn't the focus or subject of the prompt**. Examples of supporting content include things like names, preferences, future date to include in the response, and so on. Providing supporting content allows the model to respond more completely, accurately, and be more likely to include the desired information.

For example, given a very long promotional email, the model is able to extract key information. If you then add supporting content to the prompt specifying something specific you're looking for, the model can provide a more useful response. In this case **the email is the primary content, with the specifics of what you're interested in as the supporting content**.

`---`
`<insert full email here, as primary content>`
`---`
`<the next line is the supporting content>`
`Topics I'm very interested in: AI, webinar dates, submission deadlines`
`Extract the key points from the above email, and put them in a bulleted list:`

**Grounding content allows the model to provide reliable answers by providing content for the model to draw answers from**. Grounding content could be an essay or article that you then ask questions about, a company FAQ document, or information that is more recent than the data the model was trained on. If you need more reliable and current responses, or you need to reference unpublished or specific information, grounding content is highly recommended.

**Grounding content differs from primary content as it's the source of information to answer the prompt query, instead of the content being operated on for things like summarization or translation**. For example, when provided an unpublished research paper on the history of AI, it can then answer questions using that grounding content.

`---`
`<insert unpublished paper on the history of AI here, as grounding content>`
`---`
`Where and when did the field of AI start?`

This grounding data allows the model to give more accurate and informed answers that may not be part of the dataset it was trained on.

### Cues

Cues are leading words for the model to build upon, and often help shape the response in the right direction. They are often used with instructions, but not always. Cues are particularly helpful if prompting the model for code generation. Code generation is covered in more depth in a subsequent unit.

---

## Provide context with prompt engineering

By providing context to the AI model, it allows the model to better understand what you are asking for or what it should know to provide the best answer. Context can be provided in several ways.

### Request output composition

Specifying the structure of your output can have a large impact on your results. This could include something like **asking the model to cite their sources, write the response as an email, format the response as a SQL query, classify sentiment into a specific structure**, and so on. For example:

Prompt
`Write a table in markdown with 6 animals in it, with their genus and species`

This technique can be used with custom formats, such as a JSON structure:

Prompt
`Put two fictional characters into JSON of the following format`
`{`
`  firstNameFictional: `
`  jobFictional:`
`}`

### System message

**The system message is included at the beginning of a prompt and is designed to give the model instructions, perspective to answer from, or other information helpful to guide the model's response**. This system message might include **tone** or **personality**, **topics that shouldn't be included**, or **specifics (like formatting) of how to answer**.

For example, you could give it some of the following system messages:

- "I want you to act like a command line terminal. Respond to commands exactly as cmd.exe would, in one unique code block, and nothing else."
- "I want you to be a translator, from English to Spanish. Don't respond to anything I say or ask, only translate between those two languages and reply with the translated text."
- "Act as a motivational speaker, freely giving out encouraging advice about goals and challenges. You should include lots of positive affirmations and suggested activities for reaching the user's end goal."

Other example system messages are available at the top of the chat window in [Azure AI Foundry](https://ai.azure.com/portal) by selecting the **Prompt samples** button. Try defining your own system prompt that specifies a unique response, and chat with the model to see how responses differ.

The `ChatCompletion` endpoint enables including the system message by using the `System` chat role.

System message:
`"You are a casual, helpful assistant. You will talk like an American old western film character."`

System messages can significantly change the response, both in format and content. Try defining a clear system message for the model that explains exactly what kind of response you expect, and what you do or don't want it to include.

### Conversation history

Along with the system message, other messages can be provided to the model to enhance the conversation. **Conversation history enables the model to continue responding in a similar way (such as tone or formatting) and allow the user to reference previous content in subsequent queries**. This history can be provided in two ways: **from an actual chat history**, or **from a user defined example conversation**.

Chat interfaces that use OpenAI models, such as ChatGPT and the chat playground in [Azure AI Foundry](https://ai.azure.com/portal/chat), include conversation history automatically which results in a richer, more meaningful conversation. In the **Parameters** section of the chat playground, you can specify how many past messages you want included. Try reducing that to 1 or increasing to max to see how different amounts of history impact the conversation.

> Note: More conversation history included in the prompt means a larger number of input tokens are used. You will have to determine what the correct balance is for your use case, considering the token limit of the model you are using.

Chat systems can also utilize the summarization capabilities of the model to save on input tokens. An app can choose to summarize past messages and include that summary in the conversation history, then provide only the past couple messages verbatim to the model.

### Few shot learning

**Using a user defined example conversation is what is called few shot learning**, which provides the model examples of how it should respond to a given query. These examples serve to train the model how to respond.

For example, by providing the model a couple of prompts and the expected responses, it continues in the same pattern without instructing it what to do:

`User: That was an awesome experience`
`Assistant: positive`
`User: I won't do that again`
`Assistant: negative`
`User: That was not worth my time`
`Assistant: negative`
`User: You can't miss this`
`Assistant:`

If the model is provided with just `You can't miss this` with no additional context from few shot learning, the response isn't likely to be useful.

In practical terms, conversation history and few shot learning are sent to the model in the same way; each user message and assistant response is a discrete message in the message object. The `ChatCompletion` endpoint is optimized to include message history, regardless of if this message history is provided as few shot learning, or actual conversation history.

### Break down a complex task

Another technique for improved interaction is to divide complex prompts into multiple queries. This allows the model to better understand each individual part, and can improve the overall accuracy. Dividing your prompts also allows you to include the response from a previous prompt in a future prompt, and using that information in addition to the capabilities of the model to generate interesting responses.

For example, you could ask the model `Doug can ride down the zip line in 30 seconds, and takes 5 minutes to climb back up to the top. How many times can Doug ride the zip line in 17 minutes?`. The result is likely 3, which if Doug starts at the top of the zip line is because he doesn't need to return to the top after the final ride.

**A more informative answer could come from asking it multiple questions**, about the round trip time to get back to the top of the zip line, and how to account for the fact that Doug starts at the top. Breaking down this problem reveals that Doug can, in fact, ride the zip line four times.

#### Chain of thought

One useful method to help you break down your task effectively is to **ask the model to explain its chain of thought**.

Asking a model to respond with the step by step process by which it determined the response is a helpful way to understand how the model is interpreting the prompt. By doing so, you can see where the model made an incorrect logical turn and better understand how to change your prompt to avoid the error. This technique can include asking it to cite its sources, like Bing chat does (which uses a GPT-4 generation model), and giving reasoning for why it determined its answer.

The chain of thought prompting technique is best used to help you iterate and improve on your prompts to get the highest quality answer from the model.

For example, asking the model `What sport is easiest to learn but hardest to master?` results in response with an answer, and a small explanation of why. However, when prompted with `What sport is easiest to learn but hardest to master? Give a step by step approach of your thoughts, ending in your answer`, the response is a complete explanation of how it arrived at its answer.

Knowing the steps of reason the model used to form its response allows us to identify where we need to split the prompt, provide more information, or clarify instructions.

---

## Construct code from natural language

One of the capabilities of Azure OpenAI models is to generate code from natural language prompts. Tasks can range from a simple one line command to a full application. The AI models can also edit and update provided code or previous responses to complete the requested task.

The examples in this unit are using `gpt-35-turbo`.

### Write functions

Azure OpenAI models can create functions and apps in several languages by just describing what you want. For example, given the prompt `write a function for binary search in python`, you likely receive a response with the function and an explanation of the code.

The request can go further than functions, and request a series of functions or an entire app. The response provides a good starting point for the task you're trying to achieve.

### Change coding language

If you have code in one language, but need it in another, Azure OpenAI can translate that for you. For example, say you have this `print_squares` function in Python.

If you provide that code, along with the prompt `convert this code to C#`, you likely get something similar to the following function.

Notice the model understood the differences between how for loops and functions are defined in the different languages (including access modifiers in C#), and changed the code appropriately.

### Understand unknown code

Azure OpenAI models are helpful for understanding code that doesn't make sense, or may be in a language you aren't familiar with. For example, say you were given the following function (in a fictitious coding language!) and didn't know how to understand it.

When provided to the model, along with the prompt `could you explain what this code is doing?`, you get a response similar to the following:

Combining the explanation and translation into a language you're familiar with makes this functionality even more useful.

### Complete code and assist the development process

Azure OpenAI can assist developers in common software development tasks such as **writing unit tests, completing partial code, commenting code, and generating documentation**. Using AI assistance can enable developers to spend more time on complex programming and problem solving tasks.

Azure OpenAI models are capable of **generating code based on comments, function names, and partially written code**. The more context you can provide the model, the more accurate the response likely is.

For example, when given the following prompt:
`complete the following function`  
`"""`
`# calculate the average of the numbers in an array, but only if they're even  `
`def`

The model takes the comments and start of the function definition, and complete it from whatever context it has.

Similar functionality can be seen by providing partial code to the model, without any comments. Take the following prompt for example. The AI model does its best to complete the function with the most likely completion of the function.

The response likely also includes a natural language explanation of what that function is doing. In this case, it responded with a recursive implementation of the factorial function.

### Write unit tests

Azure OpenAI models can generate unit tests for functions you write to help make your code more robust. Take for example the binary search function.

Provide that function along with the prompt `write three unit tests for this function`, and you get a response similar to the following.

This functionality is even more useful if you specify the cases you want included in your prompt, which reduces the time it takes for you to write unit tests for your code.

### Add comments and generate documentation

To further improve your code, AI models can add comments and documentation for you. Take the following function as an example, which can be a little hard to understand when first reading it without any code comments.

Provide that function to the model, along with a prompt requesting that it add comments to the code, and you get a response similar to the following.

If you take it one step further, and **request documentation**, OpenAI will add a description including inputs and outputs.

### Fix bugs and improve your code

Developers sometimes can write code that mostly works, but could be improved by **fixing bugs, syntax, performance, or modularity**. Azure OpenAI models can help identify ways to improve and provide suggestions on how to write better code.

#### Fix bugs in your code

Azure OpenAI models can help fix bugs in code by analyzing the code and suggesting changes that can potentially fix the issue. This can help developers identify and resolve bugs faster and more efficiently.

For example, say you have the following function that isn't working for you.

Provide that function to the model, along with the prompt `Fix the bugs in this function`, and you get a response with the fixed code and an explanation of what was fixed.

#### Improve performance

While code developers write may work, there might be a more efficient way to accomplish the task. Here's an example of a function that calculates the sum of the first n positive integers, possibly inefficiently:

This version works correctly, but its time complexity is O(n). When provided to the model, here's the response:

This version still returns the correct result, but its time complexity is now O(1), which makes it much more efficient.

#### Refactor inefficient code

Better code is less prone to bugs and is easier to maintain, and the Azure OpenAI models can help guide developers on how to refactor their code.

Consider the following function.

This code calculates the total price of a particular item based on its name and quantity. However, the code isn't modular and can be difficult to maintain. When provided to the model with the request to refactor it, here's the response:

Along with the code, the model provides an explanation of the refactoring.

---

## [Exercise - Get started with Azure OpenAI Service](https://microsoftlearning.github.io/mslearn-openai/Instructions/Exercises/01-app-develop.html)

With the Azure OpenAI Service, developers can create chatbots and other applications that excel at understanding natural human language through the **use of REST APIs or language specific SDKs**. When working with these language models, how developers shape their prompt greatly impacts how the generative AI model will respond. Azure OpenAI models are able to tailor and format content, if requested in a clear and concise way. In this exercise, you’ll learn how to connect your application to Azure OpenAI and see how different prompts for similar content help shape the AI model’s response to better satisfy your requirements.

In the scenario for this exercise, you will perform the role of a software developer working on a wildlife marketing campaign. You are exploring how to use generative AI to improve advertising emails and categorize articles that might apply to your team. The prompt engineering techniques used in the exercise can be applied similarly for a variety of use cases.

---

## Module assessment

1. What Azure OpenAI base model can you deploy to access the capabilities of ChatGPT? gpt-35-turbo
2. Which parameter could you adjust to change the randomness or creativeness of the completions returned? Temperature
3. Which Azure AI Foundry playground is able to support conversation-in, message-out scenarios? Chat