# [Evaluate generative AI performance in Azure AI Foundry portal](https://learn.microsoft.com/en-us/training/modules/evaluate-models-azure-ai-studio/)

Evaluating copilots is essential to ensure your generative AI applications meet user needs, provide accurate responses, and continuously improve over time. Discover how to **assess and optimize the performance of your generative AI applications** using the tools and features available in the Azure AI Studio.

## Learning objectives

By the end of this module, you'll be able to:

- Understand **model benchmarks**.
- Perform manual evaluations.
- Assess your generative AI apps with **AI-assisted metrics**.
- Configure **evaluation flows** in the Azure AI Foundry portal.

## Introduction

Evaluating your generative AI apps is crucial for several reasons. 

First and foremost, it ensures **quality assurance**. By assessing your app's performance, you can identify and address any issues, ensuring that it provides accurate and relevant responses. High quality responses lead to improved user satisfaction. When users receive accurate and helpful responses, they're more likely to have a positive experience and continue using your application.

Evaluation is also essential for **continuous improvement**. By analyzing the results of your evaluations, you can identify areas for enhancement and iteratively improve your app's performance. The ongoing process of evaluation and improvement helps you stay ahead of user needs and expectations, ensuring that your app remains effective and valuable.

In this module, you learn how to **use the Azure AI Foundry portal to evaluate your generative AI apps**. While you explore some of the features of Azure AI Foundry, the focus is on understanding the importance of evaluation and how it can benefit your app development process.

---

## Assess the model performance

Evaluating your model's performance at different phases is crucial to ensure its effectiveness and reliability. Before exploring the various options you have to evaluate your model, let's explore the aspects of your application you can evaluate.

When you develop a generative AI app, you use a language model in your chat application to generate a response. To help you decide which model you want to integrate into your application, you can **evaluate the performance of an individual language model**:

![Interact with LM](https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/interact-model.png)

An input (1) is provided to a language model (2), and a response is generated as output (3). The model is then evaluated by analyzing the input, the output, and optionally comparing it to predefined expected output.

When you develop a generative AI app, you may integrate a language model into a chat flow:

![Chat flow](https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/chat-flow-diagram.png)

**A chat flow allows you to orchestrate executable flows that can combine multiple language models and Python code**. The flow expects an input (1), processes it through executing various nodes (2), and generates an output (3). You can evaluate a complete chat flow, and its individual components.

When evaluating your solution, you can **start with testing an individual model, and eventually test a complete chat flow** to validate whether your generative AI app is working as expected.

Let's explore several approaches to evaluate your model and chat flow, or generative AI app.

### Model benchmarks

**Model benchmarks are publicly available metrics across models and datasets**. These benchmarks help you understand how your model performs relative to others. Some commonly used benchmarks include:

- **Accuracy**: Compares model generated text with correct answer according to the dataset. **Result is one if generated text matches the answer exactly, and zero otherwise**.
- **Coherence**: Measures whether the model output flows smoothly, reads naturally, and resembles human-like language
- **Fluency**: Assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses.
- **GPT similarity**: Quantifies the semantic similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model.

In the Azure AI Foundry portal, you can explore the model benchmarks for all available models, before deploying a model.

### Manual evaluations

Manual evaluations involve human raters who assess the quality of the model's responses. This approach provides insights into aspects that automated metrics might miss, such as **context relevance and user satisfaction**. Human evaluators can rate responses based on criteria like relevance, informativeness, and engagement.

### AI-assisted metrics

AI-assisted metrics use advanced techniques to evaluate model performance. These metrics can include:

- **Generation quality metrics**: These metrics evaluate the overall quality of the generated text, considering factors like **creativity, coherence, and adherence to the desired style or tone**.
- **Risk and safety metrics**: These metrics assess the **potential risks and safety concerns** associated with the model's outputs. They help ensure that the model doesn't generate harmful or biased content.

### Natural language processing metrics

Natural language processing (NLP) metrics are also valuable in evaluating model performance. One such metric is the **F1-score**, which measures **the ratio of the number of shared words between the generated and ground truth answers**. The F1-score is useful for tasks like text classification and information retrieval, where **precision and recall** are important. Other common NLP metrics include:

- **BLEU**: Bilingual Evaluation Understudy metric
- **METEOR**: Metric for Evaluation of Translation with Explicit Ordering
- **ROUGE**: Recall-Oriented Understudy for Gisting Evaluation

All of these metrics are used to quantify the level of overlap in the model-generated response and the ground truth (expected response).

---

## Manually evaluate the performance of a model

During the early phases of the development of your generative AI app, you want to experiment and iterate quickly. To easily assess whether your selected language model and app, created with prompt flow, meet your requirements, you can **manually evaluate models and flows in the Azure AI Foundry portal**.

Even when your model and app are already in production, manual evaluations are a crucial part of assessing performance. As manual evaluations are done by humans, they can provide insights that automated metrics might miss.

Let's explore how you can manually evaluate your selected models and app in the Azure AI Foundry portal.

### Prepare your test prompts

To begin the manual evaluation process, it's essential to **prepare a diverse set of test prompts that reflect the range of queries and tasks your app is expected to handle**. These prompts should cover various scenarios, including common user questions, edge cases, and potential failure points. By doing so, you can comprehensively assess the app's performance and identify areas for improvement.

### Test the selected model in the chat playground

When you develop a chat application, you use a language model to generate a response. You **create a chat application by developing a prompt flow that encapsulates your chat application's logic**, which can use multiple language models to ultimately generate a response to a user question.

Before you test your app's response, you can test the selected language model's response to verify the individual model works as expected. You can test a model you deployed in the Azure AI Foundry portal by interacting with it in the **chat playground**.

**The chat playground is ideal for early development**. You can enter a prompt, see how the model responds, and tweak the prompt or system message to make improvements. After applying the changes, you can test a prompt again to evaluate whether the model's performance indeed improved.

### Evaluate multiple prompts with manual evaluations

The chat playground is an easy way to get started. When you want to manually evaluate multiple prompts more quickly, you can use the **manual evaluations** feature. This feature allows you to upload a dataset with multiple questions, and optionally add an expected response, to evaluate the model's performance on a larger test dataset.

![multi prompts](https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/manual-evaluations.png#lightbox)

You can rate the model's responses with the **thumbs up or down feature**. Based on the overall rating, you can try to **improve your model by changing input prompt, the system message, the model, or the model's parameters**.

When you use manual evaluations, you can more quickly evaluate the model's performance based on a diverse test dataset and improve the model based on the test results.

After manually evaluating an individual model, you can integrate the model into a chat application with prompt flow. **Any flow you create with prompt flow can also be evaluated manually or automatically**. Next, let's explore the evaluation of flows.

---

## Automated evaluations

Automated evaluations in Azure AI Foundry portal enable you to assess the quality and content safety performance of models, datasets, or prompt flows.

### Evaluation data

To evaluate a model, you need a dataset of prompts and responses (and optionally, expected responses as "**ground truth**"). You can compile this dataset manually or use the output from an existing application; but a useful way to get started is to **use an AI model to generate a set of prompts and responses related to a specific subject**. You can then **edit the generated prompts and responses to reflect your desired output**, and use them as ground truth to evaluate the responses from another model.

### Evaluation metrics

Automated evaluation enables you to choose **which evaluators you want to assess your model's responses**, and **which metrics those evaluators should calculate**. There are evaluators that help you measure:

- **AI Quality**: The quality of your model's responses is measured by **using AI models to evaluate them** for metrics like **coherence and relevance** and by using standard NLP metrics like **F1 score, BLEU, METEOR, and ROUGE** based on ground truth (in the form of expected response text)
- **Risk and safety**: evaluators that **assess the responses for content safety issues**, including **violence, hate, sexual content, and content related to self-harm**.

---

> Note: this article no longer exists!

## Assess the performance of your generative AI apps

When you want to create a generative AI app, you use **prompt flow** to develop the chat application. You can evaluate the performance of an app by evaluating the responses after running your flow.

### Test your flow with individual prompts

During active development, you can test the **chat flow** you're creating by using the chat feature when you have a compute session running.

When you test your flow with an individual prompt in the chat window, your flow runs with your provided input. After it successfully runs, a response is shown in the chat window. You can also explore the output of each individual node of your flow to understand how the final response was constructed.

### Automatically test your flow with evaluation flows

To evaluate a chat flow in bulk, you can run automated evaluations. You can either use the built-in automated evaluations, or you can define your custom evaluations by creating your own evaluation flow.

#### Evaluate with Microsoft-curated metrics

The built-in or **Microsoft-curated metrics** include the following metrics:

- Performance and quality:
    - **Coherence**: Measures how well the generative AI application can **produce output that flows smoothly, reads naturally, and resembles human-like language**.
    - **Fluency**: Measure the language proficiency of a generative AI application's predicted answer.
    - **GPT similarity**: Measures **the similarity between a source data (ground truth) sentence and the generated response by a generative AI application**.
    - **F1 score**: Measures **the ratio of the number of words between the generative AI application prediction and the source data** (ground truth).
    - **Groundedness**: Measures how well the generative AI application's generated answers align with information from the input source.
    - **Relevance**: Measures the extent to which the generative AI application's generated responses are pertinent and directly related to the given questions.

- Risk and safety:
    - **Self-harm-related content**: Measures the disposition of the generative AI application toward producing self-harm-related content.
    - **Hateful and unfair content**: Measures the predisposition of the generative AI application toward producing hateful and unfair content.
    - **Violent content**: Measures the predisposition of the generative AI application toward producing violent content.
    - **Sexual content**: Measures the predisposition of the generative AI application toward producing sexual content.

To evaluate your chat flow with the built-in automated evaluations, you need to:

- Create a test dataset.
- Create a new automated evaluation in the Azure AI Foundry portal.
- Select a flow or a dataset with model generated outputs.
- Select the metrics you want to evaluate on.
- Run the evaluation flow.
- Review the results.

![Metric dashboard](https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/metric-dashboard.png#lightbox)

> Tip: Learn more about [evaluation and monitoring metrics](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/evaluation-metrics-built-in)

### Create custom evaluation metrics

Alternatively, you can create your own custom evaluation flow, in which you define how your chat flow's output should be evaluated. For example, you can evaluate the output using Python code or by using a Large Language Model (LLM) node to create an AI-assisted metric. Let's explore how an evaluation flow works with a simple example.

![Eval flow](https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/evaluation-flow.png)

You can have a chat flow that takes a user's question as input (1). The flow processes the input using a language model and formats the answer with Python code (2). Finally, it returns the response as output (3).

To evaluate the chat flow, you can create an evaluation flow. The evaluation flow takes the original user question and the generated output as input (4). The flow evaluates it with a language model and uses Python code to define an evaluation metric (5), which is then returned as output (6).

When you create an evaluation flow, you can choose how to evaluate a chat flow. You can use a language model to create your own custom AI-assisted metrics. In the prompt, you can define the metric you want to measure and the grading scale the language model should use. For example, an evaluation prompt can be:

`Instructions: You are provided with the input and response of a language model that you need to evaluate on user satisfaction. User satisfaction is defined as whether the response meets the user’s question and needs, and provides a comprehensive and appropriate answer to the question. Assign each response a score of 1 to 5 for user satisfaction, with 5 being the highest score.`

After creating an evaluation flow, you can evaluate a chat flow by providing a test dataset and running the evaluation flow.

![custom-eval-flow](https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/custom-evaluation-tool.png#lightbox)

When you use a language model in an evaluation flow, you can review the results in the output trace:

![output trace](https://learn.microsoft.com/en-us/training/wwl-data-ai/evaluate-models-azure-ai-studio/media/custom-metric-result.png#lightbox)

Additionally, you can add a Python node in the evaluation flow to aggregate the results for all prompts in your test dataset and return an overall metric.

> Tip: Learn how to [develop an evaluation flow in the Azure AI Foundry portal](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/flow-develop-evaluation).

---

## [Exercise - Evaluate generative AI model performance](https://learn.microsoft.com/en-us/training/modules/evaluate-models-azure-ai-studio/5-exercise)

### [Evaluate generative AI model performance](https://microsoftlearning.github.io/mslearn-ai-studio/Instructions/07-Evaluate-prompt-flow.html)

In this exercise, you’ll use manual and automated evaluations to assess the performance of a model in the Azure AI Foundry portal.

---

## Module assessment

1. Which evaluation technique can you use to apply your own judgement about the quality of responses to a set of specific prompts? **Manual evaluation**.
2. Which evaluator compares generated responses to ground truth based on standard metrics? **F1 Score**.
3. Which evaluator metric uses an AI model to judge the structure and logical flow of ideas in a response? **Coherence**.
4. You want to compare generated responses to ground truth based on standard metrics. What kind of metrics should you specify for automated evaluations? AI quality (NLP)
5. You want to evaluate the grammatical and linguistic quality of responses. What kind of metrics should you specify for automated evaluations? AI quality (AI-assisted)

---

## Summary

In this module, you learned to:

- Understand model benchmarks.
- Perform manual evaluations.
- Perform automated evaluations.