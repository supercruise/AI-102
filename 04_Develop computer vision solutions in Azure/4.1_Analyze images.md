# [Analyze images](https://learn.microsoft.com/en-us/training/modules/analyze-images/)

With the Azure AI Vision service, you can use **pre-trained models** to **analyze images** and **extract insights and information** from them.

## Learning objectives

After completing this module, youâ€™ll be able to:

- Provision an Azure AI Vision resource.
- Use the Azure AI Vision SDK to connect to your resource.
- Write code to analyze an image.

## Introduction

Computer Vision is a branch of artificial intelligence (AI) in which software **interprets visual input, often from images or video feeds**. In Microsoft Azure, you can use the Azure AI Vision service to implement multiple computer vision scenarios, including:

- *Image analysis*
- *Optical character recognition (OCR)*
- *Face detection and analysis*
- *Video analysis*

In this module, we'll focus on **image analysis**, and explore how to build applications that use the Azure AI Vision service to **analyze and extract and infer insights from images**.

![Image](https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-images/media/computer-vision.png)

As shown in this conceptual diagram, the Azure AI Vision service provides services that you can use to analyze images and:

- Generate a **caption** for an image based on its contents.
- Suggest appropriate **tags** to associate with an image.
- **Detect and locate common objects** in an image.
- **Detect and locate people** in an image.

---

## Provision an Azure AI Vision resource

To use Azure AI Vision image analysis services, you need to provision an **Azure AI Vision resource** in your Azure subscription. You can choose from multiple provisioning options:

![options](https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-images/media/ai-vision-resources.png)

1. Create an **Azure AI Foundry** project and an associated **hub**. By default, an **Azure AI Foundry hub** includes an **Azure AI services** multi-service resource, which includes Azure AI Vision. Azure AI Foundry projects are recommended for development of AI solutions on Azure that combine generative AI, agents, and pre-built Azure AI services, or which involve collaborative development by a team of software engineers and service operators.
2. If you don't need all of the functionality in an Azure AI Foundry hub, you can create an **Azure AI services multi-service resource** in your Azure subscription. You can then use this resource to access Azure AI Vision services and other AI services through a **single endpoint** and **key**.
3. If you only need to use Azure AI Vision functionality, or you're just experimenting with the service, you can create a standalone **Computer Vision** resource in your Azure subscription. One benefit of this approach is that the standalone service provides a free tier that you can use to explore the service at no cost.

> Tip: If you're unfamiliar with Azure AI Foundry and Azure AI services, consider completing the [Plan and prepare to develop AI solutions on Azure](https://learn.microsoft.com/en-us/training/modules/prepare-azure-ai-development/) module.

### Connecting to your resource

After you've deployed your resource, you can use the [Azure AI Vision REST API](https://learn.microsoft.com/en-us/rest/api/computervision/image-analysis) or a **language-specific SDK** (such as the Python SDK or Microsoft .NET SDK) to connect to it from a client application.

Every Azure AI Vision resource provides an **endpoint** to which client applications must connect. You can find the endpoint for your resource in the Azure portal, or if you're working in an Azure AI Foundry project, in the Azure AI Foundry portal. The endpoint is in the form of a URL, and typically looks something like this:

`https://<resource_name>.cognitiveservices.azure.com/`

To connect to the endpoint, client applications must be *authenticated*. Options for **authentication** include:

- **Key-based authentication**: Client applications are authenticated by passing an **authorization key** (which you can find and manage in the portal).
- **Microsoft Entra ID authentication**: Client applications are authenticated by using a **Microsoft Entra ID token** for credentials that have permission to access the Azure AI Vision resource in Azure.

When developing and testing an application, it's common to use key-based authentication or Microsoft Entra ID authentication based on your own Azure credentials. **In production, consider using Microsoft Entra ID authentication based on a managed identity for your Azure application or use Azure Key Vault to store authorization keys securely**.

> Note: When using an Azure AI services resource in an Azure AI Foundry project, you can use the Azure AI Foundry SDK to connect to the project using Microsoft Entra ID authentication, and then retrieve the connection information for your Azure AI services resource, including the authorization key, from the project.

---

## Analyze an image

After connecting to your Azure AI Vision resource endpoint, your client application can use the service to perform image analysis tasks.

Note the following requirements for image analysis:

- The image must be presented in **JPEG, PNG, GIF, or BMP** format.
- The file size of the image must be **less than 4 megabytes (MB)**.
- The dimensions of the image must be **greater than 50 x 50 pixels**.

### Submitting an image for analysis

To analyze an image, you can use the [Analyze Image](https://learn.microsoft.com/en-us/rest/api/computervision/image-analysis/analyze-image) REST method or the equivalent method in the SDK for your preferred programming language, specifying the visual features you want to include in the analysis.

`from azure.ai.vision.imageanalysis import ImageAnalysisClient`
`from azure.ai.vision.imageanalysis.models import VisualFeatures`
`from azure.core.credentials import AzureKeyCredential`

`client = ImageAnalysisClient(
    endpoint="<YOUR_RESOURCE_ENDPOINT>",
    credential=AzureKeyCredential("<YOUR_AUTHORIZATION_KEY>")
)`

`result = client.analyze(
    image_data=<IMAGE_DATA_BYTES>, # Binary data from your image file
    visual_features=[VisualFeatures.CAPTION, VisualFeatures.TAGS],
    gender_neutral_caption=True,
)`

> Note: In this code example, the client app uses **key-based authentication**. To use Microsoft Entra ID authentication, you can use a **TokenCredential** instead of an **AzureKeyCredential**. The code example submits the image data as a binary object (which would typically be read from an image file). You can also analyze an image based on a URL by using the `analyze_from_url method`.

Available visual features are contained in the `VisualFeatures` enumeration:

- `VisualFeatures.Tags`: Identifies **tags** about the image, including objects, scenery, setting, and actions
- `VisualFeatures.Objects`: Returns the **bounding box** for each detected object
- `VisualFeatures.Caption`: Generates a **caption** of the image in natural language
- `VisualFeatures.DenseCaptions`: Generates **more detailed captions** for the objects detected
- `VisualFeatures.People`: Returns the **bounding box for detected people**
- `VisualFeatures.SmartCrops`: Returns the **bounding box of the specified aspect ratio** for the area of interest
- `VisualFeatures.Read`: Extracts **readable text**

Specifying the visual features you want analyzed in the image determines what information the response will contain. Most responses will contain a **bounding box** (if a location in the image is reasonable) or a **confidence score** (for features such as **tags or captions**).

### Processing the response

This method returns a JSON document containing the requested information. The JSON response for image analysis looks similar to this example, depending on your requested features:

`{
  "apim-request-id": "abcde-1234-5678-9012-f1g2h3i4j5k6",
  "modelVersion": "<version>",
  "denseCaptionsResult": {
    "values": [
      {
        "text": "a house in the woods",
        "confidence": 0.7055229544639587,
        "boundingBox": {
          "x": 0,
          "y": 0,
          "w": 640,
          "h": 640
        }
      },
      {
        "text": "a trailer with a door and windows",
        "confidence": 0.6675070524215698,
        "boundingBox": {
          "x": 214,
          "y": 434,
          "w": 154,
          "h": 108
        }
      }
    ]
  },
  "metadata": {
    "width": 640,
    "height": 640
  }
`}

---

## [Exercise - Analyze images](https://learn.microsoft.com/en-us/training/modules/analyze-images/5-exercise-computer-vision)

In this exercise, you use the Azure AI Vision SDK to develop a client application that analyzes images.

### [Analyze images](https://microsoftlearning.github.io/mslearn-ai-vision/Instructions/Labs/01-analyze-images.html)

Azure AI Vision is an artificial intelligence capability that enables software systems to interpret visual input by analyzing images. In Microsoft Azure, the **Vision** Azure AI service provides pre-built models for common computer vision tasks, including analysis of images to suggest captions and tags, detection of common objects and people. You can also use the Azure AI Vision service to remove the background or create a foreground matting of images.

---

## Module assessment

1. What is the purpose of Azure AI Vision Image Analysis? **To extract information about visual features in images**.
2. You want to use Azure AI Vision Image Analysis to generate suggested text descriptions for an image. Which visual feature should you specify? **DenseCaptions**

---

## Summary

In this module, you learned how to provision an Azure AI Vision resource and use it from a client application to analyze images.

You can use Azure AI Vision's image analysis capabilities in scenarios that require information extraction or inference from images. A common use case is **digital asset management (DAM)**, in which you need to *tag, catalog, and index image-based data*.

To learn more about image analysis with the Azure AI Vision service, see the [Azure AI Vision documentation](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-image-analysis).
