# [Develop a vision-enabled generative AI application](https://learn.microsoft.com/en-us/training/modules/develop-generative-ai-vision-apps/)

A picture says a thousand words, and **multimodal generative AI models** can interpret images to respond to visual prompts. Learn how to build **vision-enabled chat apps**.

## Learning objectives

After completing this module, you'll be able to:

- Deploy a **vision-enabled generative AI model** in Azure AI Foundry.
- Test an **image-based prompt** in the chat playground.
- Create a chat app that submits **image-based prompts**.

---

## Introduction

Generative AI models enable you to develop chat-based applications that reason over and respond to input. Often this input takes the form of a text-based prompt, but increasingly **multimodal models that can respond to visual input** are becoming available.

In this module, we'll discuss **vision-enabled generative AI** and explore how you can use Azure AI Foundry to create generative AI solutions that respond to prompts that include **a mix of text and image data**.

---

## Deploy a multimodal model

To handle prompts that include images, you need to deploy a **multimodal generative AI model** - in other words, a model that supports **not only text-based input, but image-based (and in some cases, audio-based) input** as well. Multimodal models available in Azure AI Foundry include (among others):

- Microsoft Phi-4-multimodal-instruct
- OpenAI gpt-4o
- OpenAI gpt-4o-mini

> Tip: To learn more about available models in Azure AI Foundry, see the [Model catalog and collections in Azure AI Foundry portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview) article in the Azure AI Foundry documentation.

### Testing multimodal models with image-based prompts

After deploying a multimodal model, you can test it in the chat playground in Azure AI Foundry portal.

![playground](https://learn.microsoft.com/en-us/training/wwl-data-ai/develop-generative-ai-vision-apps/media/image-prompt.png)

In the chat playground, you can upload an image from a local file and add text to the message to elicit a response from a multimodal model.

---

## Develop a vision-based chat app

To develop a client app that engages in **vision-based chats with a multimodal model**, you can use the same basic techniques used for text-based chats. You require a connection to the endpoint where the model is deployed, and you use that **endpoint** to submit prompts that consists of messages to the model and process the responses.

The **key difference** is that prompts for *a vision-based chat include multi-part user messages that contain both **a text (or audio where supported) content item** and **an image content item***.

The JSON representation of a prompt that includes a multi-part user message looks something like this:

~~~
{ 
    "messages": [ 
        { "role": "system", "content": "You are a helpful assistant." }, 
        { "role": "user", "content": [  
            { 
                "type": "text", 
                "text": "Describe this picture:" 
            },
            { 
                "type": "image_url",
                "image_url": {
                    "url": "https://....."
                }
            }
        ] } 
    ]
}
~~~

The image content item can be:

- A URL to an image file in a web site.
- Binary image data

When using binary data to submit a local image file, the **image_url content** takes the form of a **base64 encoded value** in a data URL format:

~~~
{
    "type": "image_url",
    "image_url": {
       "url": "data:image/jpeg;base64,<binary_image_data>"
    }
}
~~~

Depending on the model type, and where you deployed it, you can use **Microsoft Azure AI Model Inference** or **OpenAI APIs** to submit **vision-based prompts**. These libraries also provide language-specific SDKs that abstract the underlying REST APIs.

In the exercise that follows in this module, you can use the Python or .NET SDK for the **Azure AI Model Inference API** and the **OpenAI API** to develop a vision-enabled chat application.

---

## [Exercise - Develop a vision-enabled chat app](https://learn.microsoft.com/en-us/training/modules/develop-generative-ai-vision-apps/4-exercise)

### [Develop a vision-enabled chat app](https://microsoftlearning.github.io/mslearn-ai-vision/Instructions/Labs/08-gen-ai-vision.html)

In this exercise, you use the **Phi-4-multimodal-instruct** generative AI model to generate responses to prompts that include images. Youâ€™ll develop an app that provides AI assistance with fresh produce in a grocery store by using Azure AI Foundry and the Azure AI Model Inference service.

---

## Module assessment

1. Which kind of model can you use to respond to visual input? **Multimodal models**.
2. How can you submit a prompt that asks a model to analyze an image? **Submit a prompt that contains a multi-part user message, containing both text content and image content**.
3. How can you include an image in a message? **As a URL or as binary data**.

---

## Summary

In this module, you learned about **vision-enabled generative AI models** and how to implement chat solutions that include image-based input.

Vision-enabled models let you create AI solutions that can **understand images and respond to related questions or instructions**. Beyond just identifying objects in pictures, some models can also use reasoning based on what they see. For instance, they can interpret a chart or assess if an object is damaged.

> Tip: For more information about working with multimodal models in Azure AI Foundry, see [How to use image and audio in chat completions with Azure AI model inference](https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/use-chat-multi-modal) and [Quickstart: Use images in your AI chats](https://learn.microsoft.com/en-us/azure/ai-services/openai/gpt-v-quickstart).
